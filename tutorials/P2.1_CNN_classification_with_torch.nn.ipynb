{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vlamen/tue-deeplearning/blob/main/tutorials/P2.1_CNN_classification_with_torch.nn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "9_-C6GUjdJv0"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cewMXbKdJv1"
      },
      "source": [
        "\n",
        "# Tutorial Contents\n",
        "----------------\n",
        "\n",
        "In this tutorial, you will learn:\n",
        "\n",
        "- How to build your own CNN network for classification.\n",
        "\n",
        "- What is a pre-trained model. How to load it and use it for classification.\n",
        "\n",
        "- How to fine-tune the pre-trained model by transfer Learning and use it in another dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQLkD6sG5LJN"
      },
      "source": [
        "# Prepare\n",
        "We already had some of our own help functions in the tutorials P1.1. In this tutorial, we will reuse some of them for convenience.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wcD5BLbdJv2"
      },
      "source": [
        "We use code from previous tutorial to build Mnist dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "5kJyqWQI5LJN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "mnist_train = MNIST(\"2AMM10/\", train=True, download=True)\n",
        "mnist_test = MNIST(\"2AMM10/\", train=False, download=True)\n",
        "\n",
        "x_train = mnist_train.data.type(torch.FloatTensor).reshape((-1,784))/255\n",
        "y_train = mnist_train.targets\n",
        "x_valid = mnist_test.data.type(torch.FloatTensor).reshape((-1,784))/255\n",
        "y_valid = mnist_test.targets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPI8hSzf5LJX"
      },
      "source": [
        "And reuse some functions to create dataloader and train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Ns0V3feY5LJX"
      },
      "outputs": [],
      "source": [
        "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
        "    \n",
        "    output=model(xb)\n",
        "    loss = loss_func(output, yb)\n",
        "\n",
        "    if opt is not None:\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "\n",
        "    _, preds = torch.max(output, 1)\n",
        "    corrects = torch.sum(preds == yb.data)\n",
        "    \n",
        "    return loss.item(), corrects, len(xb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Y87F2kR7dJv5"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        \n",
        "        \n",
        "        # training process\n",
        "        model.train()\n",
        "\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "        sample_num=0\n",
        "        for xb, yb in train_dl:\n",
        "            \n",
        "            # forward\n",
        "            # backward and optimize only if in training phase\n",
        "            losses, corrects, nums = loss_batch(model, loss_func, xb, yb,opt)\n",
        "            \n",
        "            # statistics\n",
        "            running_loss += losses * xb.size(0)\n",
        "            running_corrects += corrects\n",
        "            sample_num+=nums\n",
        "            \n",
        "        train_loss = running_loss / sample_num\n",
        "        train_acc = running_corrects.double() / sample_num\n",
        "\n",
        "        \n",
        "        # validation process\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "            sample_num=0\n",
        "            for xb, yb in valid_dl:\n",
        "                \n",
        "                # forward\n",
        "                losses, corrects, nums = loss_batch(model, loss_func, xb, yb)\n",
        "                \n",
        "                # statistics\n",
        "                running_loss += losses * xb.size(0)\n",
        "                running_corrects += corrects\n",
        "                sample_num+=nums\n",
        "\n",
        "            val_loss = running_loss / sample_num\n",
        "            val_acc = running_corrects.double()/ sample_num\n",
        "            \n",
        "            \n",
        "        # print the results\n",
        "        print(\n",
        "            f'EPOCH: {epoch+1:0>{len(str(epochs))}}/{epochs}',\n",
        "            end=' '\n",
        "        )\n",
        "        print(f'LOSS: {train_loss:.4f}',f'ACC: {train_acc:.4f} ',end=' ')\n",
        "        print(f'VAL-LOSS: {val_loss:.4f}',f'VAL-ACC: {val_acc:.4f} ',end='\\n')\n",
        "\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "87HxLt-QdJv6"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "def get_data(train_ds, valid_ds, bs):\n",
        "    return (\n",
        "        DataLoader(train_ds, batch_size=bs, shuffle=True),\n",
        "        DataLoader(valid_ds, batch_size=bs * 2),\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Tu7j-XMedJv6"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset\n",
        "train_ds = TensorDataset(x_train, y_train)\n",
        "valid_ds = TensorDataset(x_valid, y_valid)\n",
        "\n",
        "bs=64\n",
        "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1YD4prP5LJX"
      },
      "source": [
        "\n",
        "\n",
        "# 1. Build your own CNN network\n",
        "\n",
        "\n",
        "We are now going to build our neural network with three convolutional layers.We will use Pytorch's predefined\n",
        "[Conv2d](https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d)class\n",
        "as our convolutional layer. We define a CNN with 3 convolutional layers.\n",
        "Each convolution is followed by a ReLU.  At the end, we perform an\n",
        "average pooling.  (Note that ``view`` is PyTorch's version of numpy's\n",
        "``reshape``)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "MRocTokU5LJX"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "class Mnist_CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1)\n",
        "        self.fc =    nn.Linear(10, 10)\n",
        "\n",
        "\n",
        "    def forward(self, xb):\n",
        "        xb = xb.view(-1, 1, 28, 28)\n",
        "        xb = F.relu(self.conv1(xb))\n",
        "        xb = F.relu(self.conv2(xb))\n",
        "        xb = F.relu(self.conv3(xb))\n",
        "        xb = F.avg_pool2d(xb, 4)\n",
        "        xb=xb.view(xb.size(0), -1)\n",
        "        xb = self.fc(xb)\n",
        "        return xb\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8D0PiRNR5LJX"
      },
      "source": [
        "[Momentum](https://cs231n.github.io/neural-networks-3/#sgd)is a variation on\n",
        "stochastic gradient descent that takes previous updates into account as well\n",
        "and generally leads to faster training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "UJXGsB715LJY",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 1/2 [00:04<00:04,  4.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH: 1/2 LOSS: 0.8192 ACC: 0.7198  VAL-LOSS: 0.3102 VAL-ACC: 0.9063 \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:08<00:00,  4.41s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH: 2/2 LOSS: 0.2609 ACC: 0.9213  VAL-LOSS: 0.2430 VAL-ACC: 0.9300 \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "epochs=2\n",
        "lr = 0.1\n",
        "\n",
        "model = Mnist_CNN()\n",
        "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "loss_func = F.cross_entropy\n",
        "\n",
        "\n",
        "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6x4gaoO5LJY"
      },
      "source": [
        "## nn.Sequential\n",
        "\n",
        "\n",
        "``torch.nn`` has another handy class we can use to simplify our code:\n",
        "[Sequential](https://pytorch.org/docs/stable/nn.html#torch.nn.Sequential).\n",
        "A ``Sequential`` object runs each of the modules contained within it, in a\n",
        "sequential manner. This is a simpler way of writing our neural network.\n",
        "\n",
        "To take advantage of this, we need to be able to easily define a\n",
        "**custom layer** from a given function.  For instance, PyTorch doesn't\n",
        "have a `view` layer, and we need to create one for our network. ``Lambda``\n",
        "will create a layer that we can then use when defining a network with\n",
        "``Sequential``.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "lqZo0GNi5LJY"
      },
      "outputs": [],
      "source": [
        "class Lambda(nn.Module):\n",
        "    def __init__(self, func):\n",
        "        super().__init__()\n",
        "        self.func = func\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.func(x)\n",
        "\n",
        "\n",
        "def preprocess(x):\n",
        "    return x.view(-1, 1, 28, 28)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuQcv6Gz5LJY"
      },
      "source": [
        "The model created with ``Sequential`` is simply:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "qfC-6h-q5LJY"
      },
      "outputs": [],
      "source": [
        "model = nn.Sequential(\n",
        "    Lambda(preprocess),\n",
        "    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.AvgPool2d(4),\n",
        "    Lambda(lambda x: x.view(x.size(0), -1)),\n",
        "    nn.Linear(10, 10),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "-60j469HdJv9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 1/2 [00:03<00:03,  4.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH: 1/2 LOSS: 0.8060 ACC: 0.7276  VAL-LOSS: 0.2982 VAL-ACC: 0.9072 \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:07<00:00,  3.93s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH: 2/2 LOSS: 0.2695 ACC: 0.9175  VAL-LOSS: 0.2300 VAL-ACC: 0.9310 \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "lr = 0.1\n",
        "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "epochs=2\n",
        "loss_func = F.cross_entropy\n",
        "\n",
        "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vANqvCs35LJY"
      },
      "source": [
        "## Wrapping DataLoader\n",
        "\n",
        "\n",
        "Our CNN is fairly concise, but it only works with MNIST, because:\n",
        " - It assumes the input is a 28\\*28 long vector\n",
        " - It assumes that the final CNN grid size is 4\\*4 (since that's the average\n",
        "pooling kernel size we used)\n",
        "\n",
        "Let's get rid of these two assumptions, so our model works with any 2d\n",
        "single channel image. First, we can remove the initial Lambda layer by\n",
        "moving the data preprocessing into a generator:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ocuj_ugJ5LJY"
      },
      "outputs": [],
      "source": [
        "def preprocess(x, y):\n",
        "    return x.view(-1, 1, 28, 28), y\n",
        "\n",
        "\n",
        "class WrappedDataLoader:\n",
        "    def __init__(self, dl, func):\n",
        "        self.dl = dl\n",
        "        self.func = func\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dl)\n",
        "\n",
        "    def __iter__(self):\n",
        "        batches = iter(self.dl)\n",
        "        for b in batches:\n",
        "            yield (self.func(*b))\n",
        "\n",
        "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
        "train_dl = WrappedDataLoader(train_dl, preprocess)\n",
        "valid_dl = WrappedDataLoader(valid_dl, preprocess)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbucQ66w5LJY"
      },
      "source": [
        "Next, we can replace ``nn.AvgPool2d`` with ``nn.AdaptiveAvgPool2d``, which\n",
        "allows us to define the size of the *output* tensor we want, rather than\n",
        "the *input* tensor we have. As a result, our model will work with any\n",
        "size input.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "P1NH1wL_5LJY"
      },
      "outputs": [],
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.AdaptiveAvgPool2d(1),\n",
        "    Lambda(lambda x: x.view(x.size(0), -1)),\n",
        "    nn.Linear(10, 10),\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAGQQ9Ou5LJY"
      },
      "source": [
        "Let's try it out:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "9no1yT9c5LJZ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 1/2 [00:04<00:04,  4.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH: 1/2 LOSS: 0.8101 ACC: 0.7258  VAL-LOSS: 0.3144 VAL-ACC: 0.9048 \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:08<00:00,  4.31s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH: 2/2 LOSS: 0.2350 ACC: 0.9290  VAL-LOSS: 0.1738 VAL-ACC: 0.9482 \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "lr = 0.1\n",
        "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "epochs=2\n",
        "loss_func = F.cross_entropy\n",
        "\n",
        "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjzZB3QC5LJZ"
      },
      "source": [
        "Using your GPU\n",
        "---------------\n",
        "\n",
        "First check that your GPU is working in Pytorch:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "vGcE2MSD5LJZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n"
          ]
        }
      ],
      "source": [
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJ8pywi15LJZ"
      },
      "source": [
        "And then create a device object for it:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "K1Pb7BFL5LJZ"
      },
      "outputs": [],
      "source": [
        "dev = torch.device(\n",
        "    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-2bY4d25LJZ"
      },
      "source": [
        "Let's update ``preprocess`` to move batches to the GPU:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "6eqe0UEX5LJZ"
      },
      "outputs": [],
      "source": [
        "def preprocess(x, y):\n",
        "    return x.view(-1, 1, 28, 28).to(dev), y.to(dev)\n",
        "\n",
        "\n",
        "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
        "train_dl = WrappedDataLoader(train_dl, preprocess)\n",
        "valid_dl = WrappedDataLoader(valid_dl, preprocess)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F58gA3bg5LJZ"
      },
      "source": [
        "Finally, we can move our model to the GPU.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2q08wIk15LJZ"
      },
      "source": [
        "You should find it runs faster now:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "UdZUjiyk5LJa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 1/2 [00:03<00:03,  3.95s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH: 1/2 LOSS: 0.1755 ACC: 0.9470  VAL-LOSS: 0.1929 VAL-ACC: 0.9378 \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:08<00:00,  4.30s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH: 2/2 LOSS: 0.1497 ACC: 0.9558  VAL-LOSS: 0.1340 VAL-ACC: 0.9596 \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "model.to(dev)\n",
        "\n",
        "lr = 0.1\n",
        "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "epochs=2\n",
        "loss_func = F.cross_entropy\n",
        "\n",
        "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCxYAZ5hdJv_"
      },
      "source": [
        "# 2. Pre-trained model\n",
        "---------------\n",
        "\n",
        "In practice, very few people train an entire Convolutional Network from scratch (with random initialization), because it is relatively rare to have a dataset of sufficient size. Instead, it is common to pretrain a ConvNet on a very large dataset (e.g. [ImageNet](http://www.image-net.org/), which contains 1.2 million images with 1000 categories), and then use the ConvNet either as an initialization or a fixed feature extractor for the task of interest.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2x3voprdJv_"
      },
      "source": [
        "## Download pre-trained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOsKc4dNdJwA"
      },
      "source": [
        "We can load the pre-defined model `resnet18` from `torchvision.models` and load the pre-trained parameters by setting\n",
        "`pretrained=True`. The ResNet model compromises of a bunch of ResNet blocks (Combination of convolution and identity block) and a fully connected layer. The model is trained on Imagenet dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "2I4uQboIdJwA"
      },
      "outputs": [],
      "source": [
        "from torchvision import models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "GoIp4kmmdJwA",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\1simj\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "C:\\Users\\1simj\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "model_ft = models.resnet18(pretrained=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGxRTelYdJwA"
      },
      "source": [
        "Now let's print the model structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "uvh_oS7rdJwA",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print (model_ft)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQCUbn5MdJwA"
      },
      "source": [
        "## Download example images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRM-aRtbdJwA"
      },
      "source": [
        "Since this model is already trained in imagenet, we can use it for classification directly. Let's test at few example images. We need to download the demonstrative images and labels at this [link](https://drive.google.com/drive/folders/1y8EpmXGYuZktfb0QLxF8Pg-w5HGvbtbX?usp=sharing), and upload them into your own google drive so you can see them in your colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlPVtb_tdJwA"
      },
      "source": [
        "after you upload these data to your own google drive, mount them to drive folder so we can use them in colab by:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "rdnoI66jdJwA"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[32m      2\u001b[39m get_ipython().system(\u001b[33m'\u001b[39m\u001b[33mmkdir drive\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m drive.mount(\u001b[33m'\u001b[39m\u001b[33mdrive\u001b[39m\u001b[33m'\u001b[39m)\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "!mkdir drive\n",
        "drive.mount('drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYGXraMTdJwB"
      },
      "source": [
        "Then we list all the data in your drive to see if mount successfully."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FzrhvrwUdJwB"
      },
      "outputs": [],
      "source": [
        "!ls \"drive/My Drive/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-po45bHdJwB"
      },
      "source": [
        "We load the label Json file and create a label name list `idx2label`, **note that please modify the path according to your only file location**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UytsKQFUdJwB"
      },
      "outputs": [],
      "source": [
        "# load image class labels\n",
        "import json\n",
        "idx2label = []\n",
        "\n",
        "# modify the path according to your own file location\n",
        "with open(\"drive/My Drive/2AMM10/imagenet_class_index.json\", \"r\") as read_file:\n",
        "    class_idx = json.load(read_file)\n",
        "    idx2label = [class_idx[str(k)][1] for k in range(len(class_idx))]\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M69iX72KdJwB"
      },
      "source": [
        "[`transfroms`](https://pytorch.org/vision/stable/transforms.html?highlight=torchvision%20transforms) are common image transformations provided by torchvision. They can be chained together using `Compose`. \n",
        "Here we define the `Imagenet_transform` to preprocess the images in the same way as when they are used to train the pre-trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dks_h3audJwB"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "# image preprocess\n",
        "Imagenet_transform = transforms.Compose([            \n",
        "transforms.Resize(256),                    \n",
        "transforms.CenterCrop(224),                \n",
        "transforms.ToTensor(),                     \n",
        "transforms.Normalize(                     \n",
        "mean=[0.485, 0.456, 0.406],                \n",
        "std=[0.229, 0.224, 0.225]                  \n",
        ")\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGrTkeT0dJwB"
      },
      "source": [
        "## Make classification by pre-trained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hNHHVADdJwB"
      },
      "source": [
        "Then we load the example images, and make predictions directly by our pre-trained model `resnet-18`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vFxZPPEdJwB"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "def load_img_preprocess(img_path):\n",
        "    \n",
        "    img = Image.open(img_path)\n",
        "\n",
        "    x = Imagenet_transform(img)\n",
        "    x = torch.unsqueeze(x, 0)\n",
        "    return {\"img\": img, \"x\": x}\n",
        "\n",
        "# modify the path according to your own file location\n",
        "image_fold_path= \"drive/My Drive/2AMM10/images\"\n",
        "\n",
        "elephant1 = load_img_preprocess(os.path.join(image_fold_path,'elephant1.jpg'))\n",
        "elephant2 = load_img_preprocess(os.path.join(image_fold_path,'elephant2.jpg'))\n",
        "hippo1 = load_img_preprocess(os.path.join(image_fold_path,'hippo1.jpg'))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "def show_image_predictions(img_obj):\n",
        "    plt.imshow(img_obj[\"img\"])\n",
        "    plt.show()\n",
        "    \n",
        "    model_ft.eval()\n",
        "    out = model_ft(img_obj[\"x\"])\n",
        "    percentage = torch.nn.functional.softmax(out, dim=1)[0] * 100\n",
        "    _, indices = torch.sort(out, descending=True)\n",
        "    for idx in indices[0][:5]:\n",
        "        print (\"{}, with probability: {}\".format( idx2label[idx] ,  percentage[idx].item()))\n",
        "        \n",
        "show_image_predictions(elephant1)\n",
        "show_image_predictions(elephant2)\n",
        "show_image_predictions(hippo1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1U1Lt1jdJwC"
      },
      "source": [
        "# 3. Transfer Learning "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJOGbsQRdJwC"
      },
      "source": [
        "The basic premise of [transfer learning](https://en.wikipedia.org/wiki/Transfer_learning) is simple: take a model trained on a large dataset and transfer its knowledge to a smaller dataset. We usually freeze the early convolutional layers of the network and only train the last few layers which make a prediction. The idea is the convolutional layers extract general, low-level features that are applicable across images — such as edges, patterns, gradients — and the later layers identify specific features within an image such as eyes or wheels.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jfzFcENdJwC"
      },
      "source": [
        "Thus, we  use the pre-train `Resnet-18` in section 2 and apply it to our own new task because there are universal, low-level features shared between images. Here we use MNIST as our new task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGSwuPQBdJwD"
      },
      "source": [
        "![avatar](https://miro.medium.com/max/638/1*ZkPBqU8vx2vAgcLpz9pi5g.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvrqbcpKdJwD"
      },
      "source": [
        "This Resnet-18 model has over 33 million parameters, but we’ll train only the very last few fully-connected layers. Initially, we freeze all of the model’s weights:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gwp1y3JSdJwD"
      },
      "outputs": [],
      "source": [
        "for param in model_ft.parameters():   \n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHEF7v9YdJwD"
      },
      "source": [
        "Resnet-18 is pre-trained in image net with 1000 categories, whereas MNIST has only 10. We need to modify the size of the last layer accordingly. \n",
        "Parameters of newly constructed modules have requires_grad=True by default."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6QEpDMnVdJwE"
      },
      "outputs": [],
      "source": [
        "num_ftrs = model_ft.fc.in_features\n",
        "# Here the size of each output sample is set to 10..\n",
        "model_ft.fc = nn.Linear(num_ftrs, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SG8Z__MCdJwE"
      },
      "source": [
        "Move model to GPU if appliable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrZU5a7QdJwE"
      },
      "outputs": [],
      "source": [
        "model_ft = model_ft.to(dev)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPt9v1I4dJwE"
      },
      "source": [
        "Print the parameters with `requires_grad=True`, so that double-check that only the last layer could be trained. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ptFzQXwdJwE",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "for name, param in model_ft.named_parameters():\n",
        "    if param.requires_grad:\n",
        "          print(name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YQMkTtgdJwE"
      },
      "source": [
        "The input image shape of resnet18 is `(3, 224, 224)`, whereas the image shape of MNIST dataset is `(1, 28, 28)`. We need `transforms` to preprocess our input data accordingly. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "No9oFbyadJwF"
      },
      "outputs": [],
      "source": [
        "Mnist_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.Grayscale(3),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "mnist_train = MNIST(\"2AMM10/\", train=True, download=True, transform=Mnist_transform)\n",
        "mnist_test = MNIST(\"2AMM10/\", train=False, download=True, transform=Mnist_transform)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMMNB2K9dJwF"
      },
      "source": [
        "We define a little help function here to help us transfer data to GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QjXfwndTdJwF"
      },
      "outputs": [],
      "source": [
        "def preprocess_gpu(x, y):\n",
        "    return x.to(dev), y.to(dev)\n",
        "\n",
        "train_dl, valid_dl = get_data(mnist_train, mnist_test, bs)\n",
        "train_dl = WrappedDataLoader(train_dl, preprocess_gpu)\n",
        "valid_dl = WrappedDataLoader(valid_dl, preprocess_gpu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFOz04ANdJwF"
      },
      "source": [
        "Train the model with `fit` funtion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkEoFhqWdJwF"
      },
      "outputs": [],
      "source": [
        "# this code might take a while to execute\n",
        "lr = 0.1\n",
        "opt = optim.SGD(model_ft.parameters(), lr=lr, momentum=0.9)\n",
        "epochs=2\n",
        "loss_func = F.cross_entropy\n",
        "\n",
        "fit(epochs, model_ft, loss_func, opt, train_dl, valid_dl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fnc-e1WVetnl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Ap8srIYJ5LJT",
        "gYhRl7tx5LJU",
        "8XwLSwxv5LJV",
        "U2uQ8-wW5LJV",
        "gyVJ6Zrc5LJW",
        "UXwVYkhb5LJW",
        "JC_iPa2u5LJW",
        "f1YD4prP5LJX",
        "r6x4gaoO5LJY",
        "vANqvCs35LJY",
        "cjzZB3QC5LJZ",
        "C-bD9Qul5LJa"
      ],
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
